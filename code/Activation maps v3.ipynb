{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#input_svs = '/var/shared/zelda-tcga/a32d95a7-66ec-441c-b40d-71b68adeb500/TCGA-A1-A0SK-01Z-00-DX1.A44D70FA-4D96-43F4-9DD7-A61535786297.svs'\\ninput_svs = '/var/shared/zelda-tcga/cdbde7ab-3de0-40c9-a82c-0b40fba36a38/TCGA-S3-AA12-01Z-00-DX2.4F0A4F18-41C7-4497-A7B8-5DCE610E08AD.svs'\\ntile_size = (224, 224)\\n%time loc_list, target_level, tile_size_level_target, tile_size_level_0, mask_image, thumb_img, full_thumb_img = build_tissue_mask(input_svs, target_mpp=0.6, tile_size=tile_size)\\n\\nprint 'Target level: %d' % target_level\\nprint 'Tile size target: %s' % str(tile_size_level_target)\\n\\nfig=plt.figure(figsize=(12, 8))\\nfig.add_subplot(1,3,1)\\nplt.imshow(mask_image)\\nfig.add_subplot(1,3,2)\\nplt.imshow(thumb_img)\\nfig.add_subplot(1,3,3)\\nplt.imshow(full_thumb_img)\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import numpy as np\n",
    "    \n",
    "def transparent_to_white(cur_img):\n",
    "        # Replace transparent background (from slide slicing) with white background\n",
    "        canvas = PIL.Image.new('RGBA', cur_img.size, (220,220,220,255)) # Empty canvas colour (r,g,b,a)\n",
    "        canvas.paste(cur_img, mask=cur_img) # Paste the image onto the canvas, using it's alpha channel as mask\n",
    "        canvas.thumbnail([cur_img.width, cur_img.height], PIL.Image.ANTIALIAS)\n",
    "        return canvas\n",
    "\n",
    "def build_tissue_mask(input_svs, target_mpp, tile_size):\n",
    "    from openslide import OpenSlide as op\n",
    "    import math\n",
    "    import itertools\n",
    "    \n",
    "    def on_error_return():\n",
    "        print input_svs\n",
    "        return None\n",
    " \n",
    "    slide = slide = op(input_svs)\n",
    "    \n",
    "    slide_dim_0 = slide.dimensions\n",
    "    slide_mpp = float(slide.properties['aperio.MPP'])\n",
    "    smallest_level = len(slide.level_downsamples) - 1\n",
    "    \n",
    "    if target_mpp < slide_mpp:\n",
    "        print 'Slide mpp: %f, Target mpp: %f, skipping: %s' % (slide_mpp, target_mpp, input_svs)\n",
    "        return on_error_return()\n",
    "        \n",
    "    factor_mpp =  target_mpp / slide_mpp # e.g 20x->40x : 1/(1 / 0.25) = 0.25 (need to take quarter the size of the patch)\n",
    "    tile_size_level_0 = ((int)(factor_mpp*tile_size[0]),(int)(factor_mpp*tile_size[1]))\n",
    "        \n",
    "    print tile_size_level_0, slide_dim_0, tile_size\n",
    "        \n",
    "    avail_levels = [i for i, ds in enumerate(slide.level_downsamples) if slide_mpp*ds<=target_mpp]\n",
    "    if len(avail_levels) == 0:\n",
    "        print 'No avail levels: %s' % input_svs\n",
    "        return on_error_return()\n",
    "    \n",
    "    target_level = max(avail_levels)\n",
    "    target_factor_mpp = target_mpp / (slide.level_downsamples[target_level] * slide_mpp)\n",
    "    tile_size_level_target = ((int)(target_factor_mpp*tile_size[0]),(int)(target_factor_mpp*tile_size[1]))\n",
    "    \n",
    "    smallest_level = len(slide.level_downsamples) - 1\n",
    "    tile_size_smallest_factor = slide.level_downsamples[-1]\n",
    "    tile_size_smallest = ((int) (tile_size_level_0[0] / tile_size_smallest_factor), (int) (tile_size_level_0[1] / tile_size_smallest_factor) )\n",
    "    \n",
    "    print tile_size_smallest\n",
    "    \n",
    "    col_loc = range(0,slide_dim_0[0],tile_size_level_0[0])\n",
    "    row_loc = range(0,slide_dim_0[1],tile_size_level_0[1])\n",
    "    \n",
    "    all_locs = list(itertools.product(col_loc, row_loc)) # cartesian product\n",
    "    \n",
    "    img_size = len(col_loc), len(row_loc)\n",
    "    thumb_img_size = img_size[0] * tile_size_smallest[0], img_size[1] * tile_size_smallest[1]\n",
    "    mask_image = np.zeros(shape=img_size)\n",
    "    \n",
    "    thumb_img = PIL.Image.new('RGBA', thumb_img_size, (220,220,220,255)) # Empty canvas colour (r,g,b,a)\n",
    "    full_thumb_img = slide.get_thumbnail(size=thumb_img_size)\n",
    "    \n",
    "    loc_list=[]\n",
    "    img_list = []\n",
    "    for (i,j) in all_locs:        \n",
    "        samp_image = slide.read_region(location=(i,j), level=smallest_level, size=tile_size_smallest)\n",
    "        canvas = transparent_to_white(samp_image)\n",
    "                \n",
    "        cur_img_avg = np.mean(np.array(canvas))\n",
    "        \n",
    "        is_tissue = cur_img_avg <= 220\n",
    "        x,y = i/tile_size_level_0[0], j/tile_size_level_0[1]\n",
    "        if is_tissue:\n",
    "            loc_list.append((i,j))\n",
    "            mask_image[x,y] = 255\n",
    "            thumb_img.paste(canvas, mask=canvas, box=(x*tile_size_smallest[0],y*tile_size_smallest[1])) # Paste the image onto the canvas, using it's alpha channel as mask\n",
    "    mask_image = PIL.Image.fromarray(np.uint8(mask_image.T))\n",
    "    print len(all_locs)\n",
    "    print mask_image.size[0] * mask_image.size[1]\n",
    "    return loc_list, target_level, tile_size_level_target, tile_size_level_0, mask_image, thumb_img, full_thumb_img\n",
    "    #return slide.get_thumbnail(size=mask_image.size)\n",
    "\n",
    "\"\"\"\n",
    "#input_svs = '/var/shared/zelda-tcga/a32d95a7-66ec-441c-b40d-71b68adeb500/TCGA-A1-A0SK-01Z-00-DX1.A44D70FA-4D96-43F4-9DD7-A61535786297.svs'\n",
    "input_svs = '/var/shared/zelda-tcga/cdbde7ab-3de0-40c9-a82c-0b40fba36a38/TCGA-S3-AA12-01Z-00-DX2.4F0A4F18-41C7-4497-A7B8-5DCE610E08AD.svs'\n",
    "tile_size = (224, 224)\n",
    "%time loc_list, target_level, tile_size_level_target, tile_size_level_0, mask_image, thumb_img, full_thumb_img = build_tissue_mask(input_svs, target_mpp=0.6, tile_size=tile_size)\n",
    "\n",
    "print 'Target level: %d' % target_level\n",
    "print 'Tile size target: %s' % str(tile_size_level_target)\n",
    "\n",
    "fig=plt.figure(figsize=(12, 8))\n",
    "fig.add_subplot(1,3,1)\n",
    "plt.imshow(mask_image)\n",
    "fig.add_subplot(1,3,2)\n",
    "plt.imshow(thumb_img)\n",
    "fig.add_subplot(1,3,3)\n",
    "plt.imshow(full_thumb_img)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%time patch_gen = slide_generator(input_svs, loc_list, target_level, tile_size_level_0, tile_size_level_target, tile_size)\\n\\npatch_gen, patch_gen_clone = itertools.tee(patch_gen, 2)\\n\\n%time patch_gen_clone.next()\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def slide_generator(input_svs, loc_list, target_level, tile_size_level_0, tile_size_level_target, tile_size):\n",
    "    from openslide import OpenSlide as op\n",
    "    \n",
    "    slide = slide = op(input_svs)\n",
    "    dim = slide.dimensions\n",
    "    \n",
    "    resize_factor = tuple([tile_size_level_0[i] / tile_size[i] for i in range(2)])\n",
    "    \n",
    "    for x,y in loc_list:\n",
    "        #print x,y\n",
    "        loc = x,y\n",
    "        resize = tile_size\n",
    "        tile = slide.read_region(loc, level=target_level, size=tile_size_level_target)\n",
    "        tile_resized = transparent_to_white(tile).resize(resize)\n",
    "        yield tile_resized\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "%time patch_gen = slide_generator(input_svs, loc_list, target_level, tile_size_level_0, tile_size_level_target, tile_size)\n",
    "\n",
    "patch_gen, patch_gen_clone = itertools.tee(patch_gen, 2)\n",
    "\n",
    "%time patch_gen_clone.next()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n%time batch_gen = batch_generator(patch_gen, n_patches, batch_size)\\n\\nbatch_gen, batch_gen_clone = itertools.tee(batch_gen, 2)\\n\\n%time __ = batch_gen_clone.next()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "def normalize_input(X):\n",
    "    #pixel_depth = 255.0\n",
    "    #return (X - pixel_depth / 2) / pixel_depth\n",
    "    return preprocess_input(X)\n",
    "\n",
    "def batch_generator(patch_generator, n_patches, batch_size):\n",
    "    while True:\n",
    "        batch_patches = []\n",
    "        try:\n",
    "            for i in range(n_patches*batch_size):\n",
    "                batch_patches.append(np.array(patch_generator.next().convert('RGB')))\n",
    "        except StopIteration:\n",
    "            if len(batch_patches) < n_patches:\n",
    "                raise StopIteration\n",
    "        if len(batch_patches) % n_patches != 0:\n",
    "            batch_patches = batch_patches[: - (len(batch_patches) % n_patches)]\n",
    "        batch_arr = np.array(batch_patches)\n",
    "        batch_new_shape = tuple([batch_arr.shape[0] / n_patches , n_patches] + list(batch_arr.shape[1:]))\n",
    "        yield normalize_input(np.reshape(batch_arr, batch_new_shape))\n",
    "    \n",
    "\n",
    "batch_size = 100\n",
    "n_patches = 3\n",
    "\n",
    "\"\"\"\n",
    "%time batch_gen = batch_generator(patch_gen, n_patches, batch_size)\n",
    "\n",
    "batch_gen, batch_gen_clone = itertools.tee(batch_gen, 2)\n",
    "\n",
    "%time __ = batch_gen_clone.next()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmulti_batch_gen = multi_thread_batch_gen(input_svs, loc_list, n_patches, batch_size, tile_size_level_0, tile_size, target_level, tile_size_level_target)\\n\\nmulti_batch_gen, multi_batch_gen_clone = itertools.tee(multi_batch_gen, 2)\\n\\n%time __ = multi_batch_gen_clone.next()\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_svs_region(slide, loc, tile_size_level_0, tile_size, target_level, tile_size_level_target):\n",
    "    \n",
    "    dim = slide.dimensions\n",
    "    \n",
    "    resize_factor = tuple([tile_size_level_0[i] / tile_size[i] for i in range(2)])\n",
    "    \n",
    "    resize = tile_size\n",
    "    tile = slide.read_region(loc, level=target_level, size=tile_size_level_target)\n",
    "    tile_resized = transparent_to_white(tile).resize(resize).convert('RGB')\n",
    "    return np.array(tile_resized)\n",
    "                \n",
    "def multi_thread_batch_gen(input_svs, loc_list, n_patches, batch_size, tile_size_level_0, tile_size, target_level, tile_size_level_target, max_workers=10):\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    from openslide import OpenSlide as op\n",
    "    \n",
    "    def prepare_batch(batch_patches):\n",
    "        if len(batch_patches) % n_patches != 0:\n",
    "            batch_patches = batch_patches[: - (len(batch_patches) % n_patches)]\n",
    "        batch_arr = np.array(batch_patches)\n",
    "        batch_new_shape = tuple([batch_arr.shape[0] / n_patches , n_patches] + list(batch_arr.shape[1:]))\n",
    "        return normalize_input(np.reshape(batch_arr, batch_new_shape))\n",
    "        \n",
    "    slide = op(input_svs)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "        for loc in loc_list:\n",
    "            f = executor.submit(read_svs_region, \\\n",
    "                slide, loc, tile_size_level_0, tile_size, target_level, tile_size_level_target)\n",
    "            futures.append(f)\n",
    "            \n",
    "        batch_patches = []\n",
    "        for i, f in enumerate(futures):\n",
    "            patch = f.result()\n",
    "            batch_patches.append(patch)\n",
    "            if (i+1) % (n_patches*batch_size) == 0:\n",
    "                yield prepare_batch(batch_patches)\n",
    "                batch_patches = []\n",
    "        if len(batch_patches) >= n_patches:\n",
    "            yield prepare_batch(batch_patches)\n",
    "            \n",
    "\"\"\"\n",
    "multi_batch_gen = multi_thread_batch_gen(input_svs, loc_list, n_patches, batch_size, tile_size_level_0, tile_size, target_level, tile_size_level_target)\n",
    "\n",
    "multi_batch_gen, multi_batch_gen_clone = itertools.tee(multi_batch_gen, 2)\n",
    "\n",
    "%time __ = multi_batch_gen_clone.next()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "test_batch_gen = False\n",
    "\n",
    "if test_batch_gen:\n",
    "    print len(loc_list)\n",
    "    batch_gen, batch_gen_clone = itertools.tee(multi_batch_gen, 2)\n",
    "        \n",
    "    counter = 0\n",
    "    for b in batch_gen_clone:\n",
    "        print len(b), b.shape, counter\n",
    "        counter += 1\n",
    "        if counter==5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Reshape, Layer\n",
    "\n",
    "def loss_msk_0(y_true, y_pred):\n",
    "    '''Just another crossentropy'''\n",
    "    loss = keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    mask = K.tf.reduce_sum(y_true, axis=-1) #K.tf.not_equal(K.tf.reduce_sum(y_true, axis=-1), 0 )\n",
    "    loss_masked = loss * mask #loss_masked = K.tf.boolean_mask(loss, mask)\n",
    "    return loss_masked\n",
    "\n",
    "def acc_msk_0(x_true, x_pred):\n",
    "    \"\"\"Calculate accuracy, ignore (mask) 0 labels\"\"\"\n",
    "\n",
    "    none_zero_count = K.sum(x_true)\n",
    "\n",
    "    def calc_acc():\n",
    "        trade_index = K.tf.not_equal(K.tf.reduce_sum(x_true, axis=-1), 0 )\n",
    "\n",
    "        ##remove predictions that are 0\n",
    "        x_true_tradeable = K.tf.boolean_mask(x_true, trade_index)\n",
    "        x_pred_tradeable = K.tf.boolean_mask(x_pred, trade_index)\n",
    "\n",
    "        #print x_true_tradeable.shape\n",
    "\n",
    "        accuracy = K.mean(K.equal(K.tf.argmax(x_true_tradeable,-1), K.tf.argmax(x_pred_tradeable,-1)))\n",
    "                \n",
    "        accuracy = K.tf.cast(accuracy, K.tf.float32)\n",
    "        return accuracy\n",
    "\n",
    "    #K.eval(accuracy) will cause InvalidArgumentError if none_zero_count==0, add tensorflow condition so it won't be evaluated\n",
    "    final_acc = K.tf.cond(K.tf.less(none_zero_count, 1e-7), lambda: 0.0, calc_acc)\n",
    "\n",
    "    #return K.tf.stack([none_zero_count, final_acc]) #K.tf.stack([final_acc]) #\n",
    "    return K.tf.stack([final_acc])\n",
    "\n",
    "def relu6(x):\n",
    "    return K.relu(x, max_value=6)\n",
    "\n",
    "def make_one(tensor):\n",
    "    tensor_comp = 1-tensor\n",
    "    return K.concatenate([tensor, tensor_comp], axis=-1)\n",
    "\n",
    "class FullReshape(Reshape):\n",
    "    \"\"\"Like keras Reshape layer but doing reshape on the batch dimension as well.\n",
    "        Same as doing Lambda(x : K.reshape(x, K.tf.stack(target_shape))), but lambda layers are fragile to save\\load\"\"\"\n",
    "    def __init__(self, target_shape, **kwargs):\n",
    "        super(FullReshape, self).__init__(target_shape, **kwargs)\n",
    "        self.trainable = False\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return super(FullReshape, self).compute_output_shape( (None,)+input_shape )[1:]\n",
    "    def call(self, inputs):\n",
    "        return K.reshape(inputs, K.tf.stack(self.target_shape))\n",
    "    def get_config(self):\n",
    "        #print 'ReshapeLayerConfig'\n",
    "        return super(FullReshape, self).get_config()\n",
    "class SumLayer(Layer):\n",
    "    def __init__(self, axis, keep_dims=False, **kwargs):\n",
    "        super(SumLayer, self).__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "        self.keep_dims = keep_dims\n",
    "        self.trainable = False\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        ls = input_shape[:self.axis]\n",
    "        if self.keep_dims:\n",
    "            ls += (1,)\n",
    "        ls += input_shape[self.axis+1:]\n",
    "        return ls #K.tf.stack(ls)\n",
    "    def call(self, inputs):\n",
    "        return K.sum(inputs, axis=self.axis, keepdims=self.keep_dims)\n",
    "    \n",
    "    def get_config(self):\n",
    "        #print 'SumLayerConfig'\n",
    "        config = {'axis': self.axis, 'keep_dims':self.keep_dims}\n",
    "        base_config = super(SumLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "custom_objects_scope_dict = {'relu6': relu6, 'acc_msk_0':acc_msk_0, 'loss_msk_0':loss_msk_0, \n",
    "                             'make_one':make_one,\n",
    "                             'FullReshape':FullReshape,\n",
    "                             'SumLayer':SumLayer,\n",
    "                             'keras':keras}\n",
    "\n",
    "logs_dir = '/var/shared/hist_project/Data/logs'\n",
    "checkpoint_file = 'exp_mobilenetv2_0.6mpp_3patches_sigmoid_att_cnn_new_er_eq_slidev3_dx_bn_preproc_val_e0.h5'\n",
    "#checkpoint_file = 'exp_mobilenetv2_0.6mpp_3patches_sigmoid_att_cnn_new_er_eq_slidev3_dx_bn_preproc_e69_freezeenc_10p_val_e0.h5'\n",
    "\n",
    "model = keras.models.load_model(logs_dir+\"/\"+checkpoint_file, \n",
    "                            custom_objects=custom_objects_scope_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_layer_outputs(model, batch, layers_ids=None):\n",
    "    inp = model.input                                           # input placeholder\n",
    "    if layers_ids == None:\n",
    "        layers_ids = range(len(model.layers))[1:]\n",
    "    outputs = [model.layers[layer_idx].output for layer_idx in layers_ids]          # all layer outputs\n",
    "    functor = K.function([inp, K.learning_phase()], outputs )   # evaluation function\n",
    "\n",
    "    layer_outs = functor([batch, 0.])\n",
    "    return list((model.layers[l_id].name, out) for l_id, out in  zip(layers_ids, layer_outs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfinal_frames_layer_outs = get_final_frames_layer_outs(model, loc_list, multi_batch_gen, batch_size, n_patches)\\nprint final_frames_layer_outs[0][1].shape\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from vis.utils import utils\n",
    "\n",
    "def get_final_frames_layer_outs(model, loc_list, batch_gen, batch_size, n_patches):\n",
    "    target_layer_ids = [utils.find_layer_idx(model, s) for s in ('ER_Status_patch_sigmoid','ER_Status_att_probs_tan')]\n",
    "\n",
    "    print 'number of patches: %d' % len(loc_list)\n",
    "    \n",
    "    total_batches = int(math.ceil(1.0 * len(loc_list) / batch_size/ n_patches))\n",
    "    print 'Getting output for %d batches. Batch_Size = %d' % (total_batches, batch_size)\n",
    "\n",
    "    all_frames_layer_outs = []\n",
    "    for i, batch_i in enumerate(batch_gen):\n",
    "        print 'batch %d...' % i,\n",
    "        batch_frames_layer_outs = get_model_layer_outputs(model, batch_i, target_layer_ids)\n",
    "        all_frames_layer_outs.append(batch_frames_layer_outs)\n",
    "        print 'Done'\n",
    "\n",
    "    print 'Merging...',\n",
    "\n",
    "    if len(all_frames_layer_outs) != total_batches:\n",
    "        print 'WARNING: expected %d batches but %d found' % (total_batches, len(all_frames_layer_outs))\n",
    "    \n",
    "    # Merge\n",
    "    final_frames_layer_outs = []\n",
    "    for layer_id in range(len(target_layer_ids)):\n",
    "        final_layer_out = None\n",
    "        for batch_id in range(len(all_frames_layer_outs)):\n",
    "            name, layer_out = all_frames_layer_outs[batch_id][layer_id]\n",
    "            if final_layer_out is None:\n",
    "                final_layer_out = layer_out\n",
    "            else:\n",
    "                final_layer_out = np.concatenate([final_layer_out, layer_out],axis=0)\n",
    "        final_frames_layer_outs.append( (name, final_layer_out) )\n",
    "    print 'Done'\n",
    "    return final_frames_layer_outs\n",
    "\n",
    "\"\"\"\n",
    "final_frames_layer_outs = get_final_frames_layer_outs(model, loc_list, multi_batch_gen, batch_size, n_patches)\n",
    "print final_frames_layer_outs[0][1].shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nresize = negative = False\\n\\nheat_patches_dict = create_heatmap_dict(final_frames_layer_outs, resize, negative, n_patches)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cam_from_out_vector(name, out, n_patches, resize=True, verbose=False):\n",
    "    import math\n",
    "    print name, out.shape\n",
    "    assert len(out.shape) == 3\n",
    "    num_examples = len(out)\n",
    "    roi_size = int(math.sqrt(out.shape[1]/n_patches))\n",
    "    out_reshaped = out.reshape([num_examples, n_patches, roi_size, roi_size, 1])\n",
    "    print out_reshaped.shape\n",
    "    batch_heat_patches = []\n",
    "    for i in range(num_examples): #num_examples\n",
    "        example_heat_patches = []\n",
    "        for p in range(n_patches):\n",
    "            cam = out_reshaped[i,p,:,:,0]\n",
    "            #heat_patches.append(cam)\n",
    "            #INTER_NEAREST, INTER_LINEAR (default)\n",
    "            cam_resized = cv2.resize(cam, (224, 224), interpolation=cv2.INTER_NEAREST) if resize else cam\n",
    "            example_heat_patches.append( cam_resized )\n",
    "        batch_heat_patches.append(example_heat_patches)\n",
    "        if verbose:\n",
    "            print [np.sum(s) for s in np.split(out_reshaped[i], n_patches, axis=0)]\n",
    "    \n",
    "    return np.array(batch_heat_patches)\n",
    "\n",
    "def merge_heat_maps(heat_patches_dict, key1, key2):\n",
    "    new_heat_patches = []\n",
    "    for patch1, patch2 in zip(heat_patches_dict[key1], heat_patches_dict[key2]):\n",
    "        new_heat_patches.append(patch1*patch2)\n",
    "    return np.array(new_heat_patches)\n",
    "\n",
    "def create_heatmap_dict(final_frames_layer_outs, resize, negative, n_patches):\n",
    "    heat_patches_dict = {}\n",
    "    for name, out in final_frames_layer_outs:\n",
    "        #softmax , sigmoid, multiply, probs_tan\n",
    "        if 'tan' in name:\n",
    "            heat_patches_dict[name] = cam_from_out_vector(name, (out+1)/2.0, n_patches, resize=resize)\n",
    "        elif 'sigmoid' in name:\n",
    "            if negative:\n",
    "                out = 1.0 - out\n",
    "            heat_patches_dict[name] = cam_from_out_vector(name, out, n_patches, resize=resize)\n",
    "        w,h = heat_patches_dict[name].shape[-2:]\n",
    "        heat_patches_dict[name] = heat_patches_dict[name].reshape(-1, w, h)\n",
    "        print heat_patches_dict[name].shape\n",
    "\n",
    "    heat_patches_dict['final'] = merge_heat_maps(heat_patches_dict, *heat_patches_dict.keys())\n",
    "    return heat_patches_dict\n",
    "\n",
    "\"\"\"\n",
    "resize = negative = False\n",
    "\n",
    "heat_patches_dict = create_heatmap_dict(final_frames_layer_outs, resize, negative, n_patches)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nheatmap_img = build_heatmap_mask(input_svs, loc_list, tile_size_level_0, heat_patches_dict['final'][:], output_dir='/var/shared/hist_project/Data/logs')\\nheatmap_img\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_heatmap_mask(input_svs, loc_list, tile_size_dim_0, frame_heatmaps, output_dir=None):\n",
    "    from openslide import OpenSlide as op\n",
    "    from PIL import ImageDraw, Image\n",
    "    from PIL import PngImagePlugin\n",
    "    import cv2\n",
    "    import os\n",
    "    \n",
    "    slide = op(input_svs) # Open slide, this is the slow part\n",
    "    \n",
    "    dim = slide.dimensions\n",
    "    heatmap_shape = frame_heatmaps[0].shape\n",
    "    \n",
    "    # pixel in mask to fullsize image.\n",
    "    resize_factor = (1.0 * tile_size_dim_0[0] / heatmap_shape[0], 1.0 * tile_size_dim_0[1] / heatmap_shape[1])\n",
    "    \n",
    "    mask_image_size = (int(dim[0] / resize_factor[0]), int(dim[1]/ resize_factor[1]))\n",
    "    tile_size_mask = (tile_size_dim_0[0] / resize_factor[0], tile_size_dim_0[1] / resize_factor[1])\n",
    "    \n",
    "    print dim, tile_size_dim_0, heatmap_shape\n",
    "    print resize_factor, mask_image_size\n",
    "    \n",
    "    heatmap_img = Image.new('L', mask_image_size, 0)\n",
    "    draw = ImageDraw.Draw(heatmap_img)\n",
    "    \n",
    "    for f_id, (f_loc, f_draw) in enumerate(zip(loc_list, frame_heatmaps)):\n",
    "        patch_dim_0_loc = f_loc\n",
    "        \n",
    "        patch_loc = patch_dim_0_loc[0] / resize_factor[0], patch_dim_0_loc[1] / resize_factor[1]\n",
    "        patch_end_loc = patch_loc[0] + tile_size_mask[0], patch_loc[1] + tile_size_mask[1]\n",
    "        \n",
    "        patch_loc = (int)(patch_loc[0]), (int)(patch_loc[1])\n",
    "        patch_end_loc = (int)(patch_end_loc[0]), (int)(patch_end_loc[1])\n",
    "        \n",
    "        crop_area = (patch_loc[0], patch_loc[1], patch_end_loc[0], patch_end_loc[1]) #( left, top, right, bottom )\n",
    "        \n",
    "        #heatmap\n",
    "        # draw square\n",
    "        #draw.rectangle(xy=[(crop_area[0],crop_area[1]),(crop_area[2],crop_area[3])], outline='black')\n",
    "            \n",
    "        rect_loc = (crop_area[0],crop_area[1])\n",
    "        rect_size = (crop_area[2]-crop_area[0], crop_area[3]-crop_area[1]) #right-left, bottom-top\n",
    "        f_img = cv2.resize(f_draw, rect_size)\n",
    "        #print np.max(f_img), np.min(f_img)\n",
    "        f_mask = Image.fromarray(np.uint8(f_img*255.0), mode='L')\n",
    "        \n",
    "        draw.bitmap(xy=rect_loc,bitmap=f_mask, fill=255) #fill=(5,82,5,255)) #'green'\n",
    "\n",
    "    # Save mask to png file\n",
    "    if output_dir:\n",
    "        filename = os.path.basename(input_svs)[:-4]\n",
    "        output_file = output_dir + '/' + filename + '.mask.png'\n",
    "        meta_dict = {'svs-full-size': str(dim), 'resize-factor': str(resize_factor) }\n",
    "        reserved = set(['interlace', 'gamma', 'dpi', 'transparency', 'aspect'])\n",
    "        # undocumented class\n",
    "        meta = PngImagePlugin.PngInfo()\n",
    "\n",
    "        # copy metadata into new object\n",
    "        for k,v in meta_dict.iteritems():\n",
    "            if k in reserved: continue\n",
    "            meta.add_text(k, v, 0)\n",
    "\n",
    "        heatmap_img.save(fp=output_file,format='png',compress_level=4,pnginfo=meta)  #compression (0,9). Default 6. 9=highest & slowest\n",
    "    \n",
    "    return heatmap_img\n",
    "\n",
    "\"\"\"\n",
    "heatmap_img = build_heatmap_mask(input_svs, loc_list, tile_size_level_0, heat_patches_dict['final'][:], output_dir='/var/shared/hist_project/Data/logs')\n",
    "heatmap_img\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbuild_final_mask_image(mask_image, heatmap_img)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_final_mask_image(mask_image, heatmap_img):\n",
    "    target_size = heatmap_img.size\n",
    "    print target_size\n",
    "\n",
    "    mask_image_resized = mask_image.resize(target_size)\n",
    "    B, G, R = np.array(mask_image_resized), np.array(heatmap_img), np.zeros(target_size).T\n",
    "\n",
    "    print R.shape, G.shape, B.shape\n",
    "    rgb = np.uint8(np.stack([R, G, B], axis=2))\n",
    "    rgbimg = PIL.Image.fromarray(rgb, mode='RGB')\n",
    "    return rgbimg\n",
    "\n",
    "\"\"\"\n",
    "build_final_mask_image(mask_image, heatmap_img)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ninput_svs = '/var/shared/zelda-tcga/cdbde7ab-3de0-40c9-a82c-0b40fba36a38/TCGA-S3-AA12-01Z-00-DX2.4F0A4F18-41C7-4497-A7B8-5DCE610E08AD.svs'\\nfinal_mask_img = build_mask_from_svs_file(model, input_svs)\\nfinal_mask_img\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_mask_from_svs_file(model, input_svs):\n",
    "    batch_size = 100\n",
    "    n_patches = 3\n",
    "    tile_size=(224,224) \n",
    "    target_mpp=0.6\n",
    "    resize = negative = False\n",
    "    \n",
    "    # Build tissue mask\n",
    "    loc_list, target_level, tile_size_level_target, tile_size_level_0, mask_image, thumb_img, full_thumb_img = build_tissue_mask(input_svs, target_mpp=target_mpp, tile_size=tile_size)\n",
    "    \n",
    "    # Build patch batch gen\n",
    "    \n",
    "    # 1 Thread\n",
    "    ####patch_gen = slide_generator(input_svs, loc_list, target_level, tile_size_level_0, tile_size_level_target, tile_size)\n",
    "    ####batch_gen = batch_generator(patch_gen, n_patches, batch_size)\n",
    "    # N Threads\n",
    "    batch_gen = multi_thread_batch_gen(input_svs, loc_list, n_patches, batch_size, tile_size_level_0, tile_size, target_level, tile_size_level_target, max_workers=10)\n",
    "    \n",
    "    # Run Model on patches\n",
    "    final_frames_layer_outs = get_final_frames_layer_outs(model, loc_list, batch_gen, batch_size, n_patches)\n",
    "    \n",
    "    # Create heatmap dict\n",
    "    heatmap_dict = create_heatmap_dict(final_frames_layer_outs, resize, negative, n_patches)\n",
    "    \n",
    "    # Create heatmap mask\n",
    "    heatmap_img = build_heatmap_mask(input_svs, loc_list, tile_size_level_0, heatmap_dict['final'][:])\n",
    "\n",
    "    # Merge tissue mask and heatmap mask\n",
    "    final_mask_img = build_final_mask_image(mask_image, heatmap_img)\n",
    "    \n",
    "    return final_mask_img\n",
    "\n",
    "\"\"\"\n",
    "input_svs = '/var/shared/zelda-tcga/cdbde7ab-3de0-40c9-a82c-0b40fba36a38/TCGA-S3-AA12-01Z-00-DX2.4F0A4F18-41C7-4497-A7B8-5DCE610E08AD.svs'\n",
    "final_mask_img = build_mask_from_svs_file(model, input_svs)\n",
    "final_mask_img\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patients: 9565, slides: 26150\n",
      "files to slice : 26150\n"
     ]
    }
   ],
   "source": [
    "# Find list of files to slice\n",
    "\n",
    "import pandas as pd\n",
    "import distutils.dir_util\n",
    "\n",
    "target_mpp = 0.6\n",
    "group_name = 'TCGA'\n",
    "\n",
    "def find_file_recursive(folder, file_ext='*.svs'):\n",
    "    import fnmatch\n",
    "    import os\n",
    "\n",
    "    matches = []\n",
    "    for root, dirnames, filenames in os.walk(folder, followlinks=True):\n",
    "        for filename in fnmatch.filter(filenames, file_ext):\n",
    "            matches.append(os.path.join(root, filename))\n",
    "    return matches\n",
    "\n",
    "def findnth(haystack, needle, n):\n",
    "    \"\"\"Find Nth occurance of string in another string\"\"\"\n",
    "    parts= haystack.split(needle, n+1)\n",
    "    if len(parts)<=n+1:\n",
    "        return -1\n",
    "    return len(haystack)-len(parts[-1])-len(needle)\n",
    "\n",
    "def prepare_data(level1_folder, group):\n",
    "    import os\n",
    "    data = {}\n",
    "    svs_files = find_file_recursive(level1_folder, '*.svs')\n",
    "    for full_filename in svs_files:\n",
    "        filename = os.path.basename(full_filename)\n",
    "        if group not in filename:\n",
    "            print 'file without case, skipping : %s' % filename\n",
    "            continue\n",
    "        if group=='TCGA':\n",
    "            case_barcode = filename[:findnth(filename,'-',2)]\n",
    "        elif group=='GTEX':\n",
    "            case_barcode = filename[:findnth(filename,'-',1)]\n",
    "        else:\n",
    "            raise Error('Unknown group name %s' % group)\n",
    "        patient_slides = data.get(case_barcode,[])\n",
    "        patient_slides.append(full_filename)\n",
    "        data[case_barcode] = patient_slides\n",
    "    return data\n",
    "\n",
    "def filenames_no_ext(fullname_list):\n",
    "    return [filename_no_ext(f) for f in fullname_list]\n",
    "def filename_no_ext(f):\n",
    "    return os.path.basename(f)[:-4]\n",
    "\n",
    "# Flatten function\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "data = prepare_data('/var/shared/zelda-tcga', group_name)\n",
    "\n",
    "print 'patients: %d, slides: %d' % (len(data), len(flatten(data.values())))\n",
    "\n",
    "files_to_slice = flatten([data[k] for k in set(data.keys())])\n",
    "print 'files to slice : %d' % len(files_to_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/shared/hist_project/Data/0.6mpp_masks\n",
      "patches exist (tif) : 328\n",
      "delta to slice: 25819\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "level2_folder = '/var/shared/hist_project/Data/Level2'\n",
    "out_folder = '/var/shared/hist_project/Data/0.6mpp_masks'\n",
    "distutils.dir_util.mkpath(out_folder) # Create folders if missing\n",
    "\n",
    "print out_folder\n",
    "\n",
    "files_tiff_exist = [os.path.basename(f)[:-9] for f in os.listdir(out_folder)]\n",
    "files_tiff_exist = set(files_tiff_exist)\n",
    "print 'patches exist (tif) : %d' % len(files_tiff_exist)\n",
    "\n",
    "files_to_slice_new = [f for f in files_to_slice if filename_no_ext(f) not in files_tiff_exist]\n",
    "print 'delta to slice: %d' % len(files_to_slice_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2697 2393\n",
      "Duplicate svs files: 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hit_map = {}\n",
    "\n",
    "to_patch_csv = pd.read_csv('/home/maor/to_patch_df.csv')\n",
    "\n",
    "to_patch_csv = to_patch_csv.loc[to_patch_csv['PrimarySite']=='Breast']\n",
    "to_patch_csv = to_patch_csv.loc[to_patch_csv['Sample_type']=='Tumor']\n",
    "\n",
    "to_patch_set = set(to_patch_csv['Slide_name'])\n",
    "files_to_slice_filtered = []\n",
    "for full_svs_file in files_to_slice_new:\n",
    "    slide_name = os.path.basename(full_svs_file)[:-4]\n",
    "    if slide_name in to_patch_set:\n",
    "        files_to_slice_filtered.append(full_svs_file)\n",
    "        if slide_name not in hit_map:\n",
    "            hit_map[slide_name] = []\n",
    "        hit_map[slide_name].append(full_svs_file)\n",
    "        \n",
    "dup_map = {k:v for k,v in hit_map.iteritems() if len(v)> 1}\n",
    "print len(to_patch_set), len(files_to_slice_filtered)\n",
    "#print dup_map # downloaded duplicate slides...\n",
    "print 'Duplicate svs files: %d' % len(dup_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(536, 536) (108493, 88844) (224, 224)\n",
      "(16, 16)\n"
     ]
    }
   ],
   "source": [
    "def mask_all_svs(model, svs_files, output_dir):\n",
    "    if not os.path.isdir(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    for svs_file in svs_files:\n",
    "        mask_filename = os.path.basename(svs_file)[:-4] + '.mask.png'\n",
    "        \n",
    "        mask_img =  build_mask_from_svs_file(model, svs_file)\n",
    "        \n",
    "        output_file = output_dir + '/' + mask_filename\n",
    "        mask_img.save(fp=output_file,format='png',compress_level=4)\n",
    "        \n",
    "mask_all_svs(model, files_to_slice_filtered, out_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
